{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Fraud From Enron Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Udacity Nanodegree Project 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####By Joe Nyzio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Understanding the Dataset and Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Address the most important characteristics of the dataset and use these characteristics to inform analysis.  Include total number of data points, allocation across classes, and number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Address the most important characteristics of the dataset and use the characteristics to inform analysis.  \n",
    "- Total no. of data points\n",
    "- allocation across classes (POI/non-POI)\n",
    "-no. of features used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    starter code for exploring the Enron dataset (emails + finances) \n",
    "    loads up the dataset (pickled dict of dicts)\n",
    "\n",
    "    the dataset has the form\n",
    "    enron_data[\"LASTNAME FIRSTNAME MIDDLEINITIAL\"] = { features_dict }\n",
    "\n",
    "    {features_dict} is a dictionary of features associated with that person\n",
    "    you should explore features_dict as part of the mini-project,\n",
    "    but here's an example to get you started:\n",
    "\n",
    "    enron_data[\"SKILLING JEFFREY K\"][\"bonus\"] = 5600000\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "enron_data = pickle.load(open(\"../data/final_project_dataset.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many data points (people) are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 146 people in the dataset.\n"
     ]
    }
   ],
   "source": [
    "people = len(enron_data)\n",
    "print \"There are \" + str(people) + \" people in the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many features are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21 features in the dataset.\n"
     ]
    }
   ],
   "source": [
    "features = len(enron_data['ALLEN PHILLIP K'])\n",
    "print \"There are \" + str(features) + \" features in the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many poi's are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 poi's in the dataset.\n"
     ]
    }
   ],
   "source": [
    "def poi_counter(file):\n",
    "    count = 0 \n",
    "    for person in file:\n",
    "        if file[person]['poi'] == True:\n",
    "            count += 1\n",
    "    print \"There are \" + str(count) + \" poi's in the dataset.\"\n",
    "\n",
    "poi_counter(enron_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../final_project/poi_names.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4f989cc050a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../final_project/poi_names.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpoi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"There were \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" poi's total.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../final_project/poi_names.txt'"
     ]
    }
   ],
   "source": [
    "ofile = open('../final_project/poi_names.txt','r')\n",
    "rfile = ofile.readlines()\n",
    "poi = len(rfile[2:])\n",
    "print \"There were \" + str(poi) + \" poi's total.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(enron_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdf = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Outlier Investigation (related mini-project : Lesson 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify outliers in the financial data, and explain how they are removed or otherwise handled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POI Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from sklearn import linear_model\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "\n",
    "### read in data dictionary, convert to numpy array\n",
    "data_dict = pickle.load( open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "features = [\"bonus\", \"salary\"]\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "\n",
    "### your code below\n",
    "print data.max()\n",
    "for point in data:\n",
    "    bonus = point[0]\n",
    "    salary = point[1]\n",
    "    matplotlib.pyplot.scatter( bonus, salary )\n",
    "\n",
    "matplotlib.pyplot.xlabel(\"bonus\")\n",
    "matplotlib.pyplot.ylabel(\"salary\")\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one major outlier in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_outliers = []\n",
    "for key in data_dict:\n",
    "    val = data_dict[key]['bonus']\n",
    "    if val == 'NaN':\n",
    "        continue\n",
    "    b_outliers.append((key,int(val)))\n",
    "\n",
    "pprint(sorted(b_outliers,key=lambda x:x[1],reverse=True)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_outliers = []\n",
    "for key in data_dict:\n",
    "    val = data_dict[key]['salary']\n",
    "    if val == 'NaN':\n",
    "        continue\n",
    "    s_outliers.append((key,int(val)))\n",
    "\n",
    "pprint(sorted(s_outliers,key=lambda x:x[1],reverse=True)[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 'TOTAL' column as the major outlier in this dataset.  This will change our results and needs to be removed.  The other outlier is JEFFREY SKILLING.  This is not a mistake or unwanted point.  It is the boss and will definitely be an important point to keep in our investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "\n",
    "### read in data dictionary, convert to numpy array\n",
    "data_dict = pickle.load( open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "features = [\"salary\", \"bonus\"]\n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "\n",
    "### your code below\n",
    "print data.max()\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    matplotlib.pyplot.scatter( salary, bonus )\n",
    "\n",
    "matplotlib.pyplot.xlabel(\"salary\")\n",
    "matplotlib.pyplot.ylabel(\"bonus\")\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to investigate the variables 'from_poi_to_this_person' and 'from_this_person_to_poi'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "\n",
    "### read in data dictionary, convert to numpy array\n",
    "data_dict = pickle.load( open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "features = [\"from_this_person_to_poi\", \"from_poi_to_this_person\"]\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "\n",
    "### your code below\n",
    "print data.max()\n",
    "for point in data:\n",
    "    from_this_person_to_poi = point[0]\n",
    "    from_poi_to_this_person = point[1]\n",
    "    matplotlib.pyplot.scatter( from_this_person_to_poi, from_poi_to_this_person )\n",
    "\n",
    "matplotlib.pyplot.xlabel(\"from_this_person_to_poi\")\n",
    "matplotlib.pyplot.ylabel(\"from_poi_to_this_person\")\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few outliers, 2 major ones, who are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_poi_outliers = []\n",
    "for key in data_dict:\n",
    "    val = data_dict[key]['from_this_person_to_poi']\n",
    "    if val == 'NaN':\n",
    "        continue\n",
    "    to_poi_outliers.append((key,int(val)))\n",
    "\n",
    "pprint(sorted(to_poi_outliers,key=lambda x:x[1],reverse=True)[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are actual people, not mistakes, likely bosses placed in charge of communications, I'll keep these outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from_poi_outliers = []\n",
    "for key in data_dict:\n",
    "    val = data_dict[key]['from_poi_to_this_person']\n",
    "    if val == 'NaN':\n",
    "        continue\n",
    "    from_poi_outliers.append((key,int(val)))\n",
    "\n",
    "pprint(sorted(from_poi_outliers,key=lambda x:x[1],reverse=True)[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are also people in the dataset and will be important for identifying POI's.  Looks like LAVORATO JOHN J is a big deal in communications throughout Enron.  Bet he made a big bonus for his efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm just going to check out bonus and salary for these top communicators to see if they appear relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data['LAVORATO JOHN J']['bonus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data['LAVORATO JOHN J']['salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Janet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data['DIETRICH JANET R']['bonus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data['DIETRICH JANET R']['salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data['DELAINEY DAVID W']['bonus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_data['DELAINEY DAVID W']['salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll only be removing 'TOTAL' for my final file.  I will use the following function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile remove_outlier.py\n",
    "\n",
    "def remove_outlier(dict_object, keys):\n",
    "    \"\"\" removes a list of keys from a dict object \"\"\"\n",
    "    for key in keys:\n",
    "        dict_object.pop(key, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimize Feature Selection/Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create new features (related mini-project: Lesson 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My new feature is an attempt to reduce of a persons contact information to single variable.  I want this variable to be the ratio between the workers emails to poi's and non poi's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting new_feature.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile new_feature.py\n",
    "#!/usr/bin/python\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat\n",
    "from feature_format import targetFeatureSplit\n",
    "\n",
    "def add_poi_interaction(data_dict, features_list):\n",
    "    \"\"\" mutates data dict to add proportion of email interaction with pois \"\"\"\n",
    "    fields = ['to_messages', 'from_messages',\n",
    "              'from_poi_to_this_person', 'from_this_person_to_poi']\n",
    "    for record in data_dict:\n",
    "        person = data_dict[record]\n",
    "        is_valid = True\n",
    "        for field in fields:\n",
    "            if person[field] == 'NaN':\n",
    "                is_valid = False\n",
    "        if is_valid:\n",
    "            total_messages = person['to_messages'] +\\\n",
    "                             person['from_messages']\n",
    "            poi_messages = person['from_poi_to_this_person'] +\\\n",
    "                           person['from_this_person_to_poi']\n",
    "            person['poi_interaction'] = float(poi_messages) / total_messages\n",
    "        else:\n",
    "            person['poi_interaction'] = 'NaN'\n",
    "    features_list += ['poi_interaction']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    features_list = ['poi',\n",
    "                     'bonus',\n",
    "                     'deferral_payments',\n",
    "                     'deferred_income',\n",
    "                     'director_fees',\n",
    "                     'exercised_stock_options',\n",
    "                     'expenses',\n",
    "                     'loan_advances',\n",
    "                     'long_term_incentive',\n",
    "                     'other',\n",
    "                     'restricted_stock',\n",
    "                     'restricted_stock_deferred',\n",
    "                     'salary',\n",
    "                     'total_payments',\n",
    "                     'total_stock_value',\n",
    "                     'from_messages',\n",
    "                     'from_poi_to_this_person',\n",
    "                     'from_this_person_to_poi',\n",
    "                     'shared_receipt_with_poi',\n",
    "                     'to_messages']\n",
    "\n",
    "    add_poi_interaction(data_dict, features_list)\n",
    "    # visualize(data_dict, 'total_stock_value', 'poi_interaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing poi_ratio.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile poi_ratio.py\n",
    "\n",
    "def add_poi_ratio(data_dict, features_list):\n",
    "    \n",
    "    values = ['from_poi_to_this_person', \n",
    "              'from_this_person_to_poi',\n",
    "              'to_messages', \n",
    "              'from_messages']\n",
    "    \n",
    "    for record in data_dict:\n",
    "        person = data_dict[record]\n",
    "        valid = True\n",
    "        for value in values:\n",
    "            if person[value] == 'NaN':\n",
    "                valid = False\n",
    "                \n",
    "        if valid:\n",
    "            poi = person['from_poi_to_this_person'] +\\\n",
    "                  person['from_this_person_to_poi']\n",
    "            total = person['to_messages'] +\\\n",
    "                    person['from_messages']\n",
    "\n",
    "            person['poi_ratio'] = float(poi) / total\n",
    "        else:\n",
    "            person['poi_ratio'] = 'NaN'\n",
    "    features_list += ['poi_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Intelligently select features (related mini-project : Lesson 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My features were chosen the best results from the comparison of hand picked combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFeature options cheat sheet\\n\\nbonus                     \\ndeferral_payments       \\ndeferred_income            \\ndirector_fees              \\nemail_address               \\nexercised_stock_options    \\nexpenses                    \\nfrom_messages        \\nfrom_poi_to_this_person     \\nfrom_this_person_to_poi  \\nloan_advances                \\nlong_term_incentive          \\nother                        \\npoi                        \\nrestricted_stock            \\nrestricted_stock_deferred    \\nsalary                       \\nshared_receipt_with_poi     \\nto_messages                  \\ntotal_payments               \\ntotal_stock_value            \\n\\nfeatures_list1 = ['poi', \\n                 'salary', \\n                 'bonus'] #1 \\n\\nfeatures_list2 = ['poi', \\n                 'salary', \\n                 'bonus', \\n                 'from_this_person_to_poi'] #2\\n\\nfeatures_list3 = ['poi', \\n                  'salary', \\n                  'from_poi_to_this_person', \\n                  'from_this_person_to_poi'] #3\\n\\nfeatures_list4 = ['poi',\\n                  'from_this_person_to_poi', \\n                  'from_poi_to_this_person', \\n                  'shared_receipt_with_poi'] #4\\n\\nfeatures_list5 = ['poi',\\n                  'from_this_person_to_poi'] #5\\n\\nGaussian test\\n1 = Accuracy: 0.24480\\tPrecision: 0.18368\\tRecall: 0.80600\\n2 = Accuracy: 0.23282\\tPrecision: 0.15703\\tRecall: 0.73700\\n3 = Accuracy: 0.58418\\tPrecision: 0.16291\\tRecall: 0.31100\\n4 = Accuracy: 0.71678\\tPrecision: 0.01757\\tRecall: 0.00500\\n5 = Accuracy: 0.63971\\tPrecision: 0.12392\\tRecall: 0.04300\\tF1: 0.06385\\n\\n#4 = highest accuracy, try different classifier tunes to improve precision and recall\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Feature options cheat sheet\n",
    "\n",
    "bonus                     \n",
    "deferral_payments       \n",
    "deferred_income            \n",
    "director_fees              \n",
    "email_address               \n",
    "exercised_stock_options    \n",
    "expenses                    \n",
    "from_messages        \n",
    "from_poi_to_this_person     \n",
    "from_this_person_to_poi  \n",
    "loan_advances                \n",
    "long_term_incentive          \n",
    "other                        \n",
    "poi                        \n",
    "restricted_stock            \n",
    "restricted_stock_deferred    \n",
    "salary                       \n",
    "shared_receipt_with_poi     \n",
    "to_messages                  \n",
    "total_payments               \n",
    "total_stock_value            \n",
    "\n",
    "features_list1 = ['poi', \n",
    "                 'salary', \n",
    "                 'bonus'] #1 \n",
    "\n",
    "features_list2 = ['poi', \n",
    "                 'salary', \n",
    "                 'bonus', \n",
    "                 'from_this_person_to_poi'] #2\n",
    "\n",
    "features_list3 = ['poi', \n",
    "                  'salary', \n",
    "                  'from_poi_to_this_person', \n",
    "                  'from_this_person_to_poi'] #3\n",
    "\n",
    "features_list4 = ['poi',\n",
    "                  'from_this_person_to_poi', \n",
    "                  'from_poi_to_this_person', \n",
    "                  'shared_receipt_with_poi'] #4\n",
    "\n",
    "features_list5 = ['poi',\n",
    "                  'from_this_person_to_poi'] #5\n",
    "\n",
    "Gaussian test\n",
    "1 = Accuracy: 0.24480\tPrecision: 0.18368\tRecall: 0.80600\n",
    "2 = Accuracy: 0.23282\tPrecision: 0.15703\tRecall: 0.73700\n",
    "3 = Accuracy: 0.58418\tPrecision: 0.16291\tRecall: 0.31100\n",
    "4 = Accuracy: 0.71678\tPrecision: 0.01757\tRecall: 0.00500\n",
    "5 = Accuracy: 0.63971\tPrecision: 0.12392\tRecall: 0.04300\tF1: 0.06385\n",
    "\n",
    "#4 = highest accuracy, try different classifier tunes to improve precision and recall\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Properly scale features (related mini-project Lessons 1-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See response questions.  Question 2/Scaling Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Pick and tune an Algorithm (related mini-project Lesson 2,3,13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Pick an algorithm (related to mini project Lessons 1-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the process I went through while picking and tuning the algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn import tree\n",
    "\n",
    "#clf = GaussianNB() \n",
    "#4 = Accuracy: 0.71678\tPrecision: 0.01757\tRecall: 0.00500\n",
    "#clf = sklearn.tree.DecisionTreeClassifier()\n",
    "#Accuracy: 0.73856\tPrecision: 0.39827\tRecall: 0.34550\n",
    "\n",
    "#Decision tree showing immediate gains in all categories over GaussianNB.  \n",
    "#Will use for tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Tune the algorithm (related to mini project Lessons 2,3,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script.\n",
    "### Because of the small size of the dataset, the script uses stratified\n",
    "### shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(max_depth = 1) #1                       \n",
    "#Accuracy: 0.81478\tPrecision: 0.93701\tRecall: 0.17850  \n",
    "#clf = sklearn.tree.DecisionTreeClassifier(max_depth = 5) #2\n",
    "# Accuracy: 0.73078\tPrecision: 0.37174\tRecall: 0.30650\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(max_depth = 10) #3\n",
    "#Accuracy: 0.73756\tPrecision: 0.39716\tRecall: 0.34950\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(criterion = 'entropy') #4\n",
    "#Accuracy: 0.73989\tPrecision: 0.40606\tRecall: 0.36850\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(criterion ='entropy',max_depth = 1) #5\n",
    "#Accuracy: 0.81222\tPrecision: 0.93296\tRecall: 0.16700\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(criterion ='entropy',max_depth = 10) #6\n",
    "#Accuracy: 0.74000\tPrecision: 0.40207\tRecall: 0.34900\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(min_samples_leaf=5) #7\n",
    "#Accuracy: 0.72878\tPrecision: 0.38036\tRecall: 0.35050   \n",
    "#clf = sklearn.tree.DecisionTreeClassifier(min_samples_leaf=2) #8\n",
    "#Accuracy: 0.73822\tPrecision: 0.38648\tRecall: 0.30300\n",
    "\n",
    "#Little improvement from #6 but consistently better without too much complexity\n",
    "#Favorite is #5 but doesn't meet .3 recall. Use #5 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5. Validate and Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See response question #6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. My final poi_id.py file (playground solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named poi_interaction",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f46fc4f96053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpoi_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpoi_interaction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnew_feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named poi_interaction"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import test_classifier, dump_classifier_and_data\n",
    "import sklearn\n",
    "import remove_outlier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import poi_ratio\n",
    "import poi_interaction\n",
    "import new_feature\n",
    "\n",
    "features_list = [\n",
    "    'from_messages',\n",
    "    'from_poi_to_this_person',\n",
    "    'from_this_person_to_poi',\n",
    "    'shared_receipt_with_poi',\n",
    "    'to_messages',\n",
    "    'bonus',\n",
    "    'deferral_payments',\n",
    "    'deferred_income',\n",
    "    'director_fees',\n",
    "    'exercised_stock_options',\n",
    "    'expenses',\n",
    "    'loan_advances',\n",
    "    'long_term_incentive',\n",
    "    'other',\n",
    "    'restricted_stock',\n",
    "    'restricted_stock_deferred',\n",
    "    'salary',\n",
    "    'total_payments',\n",
    "    'total_stock_value',\n",
    "    ]\n",
    "\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "#features_list = ['poi', 'salary', 'bonus'] #1 \n",
    "#features_list = ['poi', 'salary', 'bonus', 'from_this_person_to_poi'] #2\n",
    "#features_list = ['poi', 'salary', 'from_poi_to_this_person', 'from_this_person_to_poi'] #3\n",
    "features_list = ['poi','from_this_person_to_poi', 'from_poi_to_this_person', 'shared_receipt_with_poi','poi_interaction'] #4\n",
    "#features_list = ['poi','from_this_person_to_poi'] #5\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "outliers = ['TOTAL']\n",
    "remove_outlier.remove_outlier(data_dict, outliers)\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "#Make Copies\n",
    "my_dataset = copy(data_dict)\n",
    "features_list = copy(features_list)\n",
    "\n",
    "# add new feature new features\n",
    "new_feature.add_poi_interaction(my_dataset, my_feature_list)\n",
    "\n",
    "\n",
    "#Used gaussianNB to quickly determine features with highest accuracy\n",
    "\n",
    "\n",
    "#Gaussian test\n",
    "#1 = Accuracy: 0.24480\tPrecision: 0.18368\tRecall: 0.80600\n",
    "#2 = Accuracy: 0.23282\tPrecision: 0.15703\tRecall: 0.73700\n",
    "#3 = Accuracy: 0.58418\tPrecision: 0.16291\tRecall: 0.31100\n",
    "#4 = Accuracy: 0.71678\tPrecision: 0.01757\tRecall: 0.00500\n",
    "#5 = Accuracy: 0.63971\tPrecision: 0.12392\tRecall: 0.04300\tF1: 0.06385\n",
    "\n",
    "#4 = highest accuracy, try different classifier tunes to improve precision and recall\n",
    "\n",
    "#Print features\n",
    "print \"{0} selected features: {1}\\n\".format(len(features_list) - 1, features_list[1:])\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "\n",
    "#clf = GaussianNB() \n",
    "#4 = Accuracy: 0.71678\tPrecision: 0.01757\tRecall: 0.00500 <-excludes made up feature\n",
    "#5 = Accuracy: 0.71489\tPrecision: 0.02833\tRecall: 0.00850 <- includes made up feature\n",
    "clf = sklearn.tree.DecisionTreeClassifier()\n",
    "#Accuracy: 0.73856\tPrecision: 0.39827\tRecall: 0.34550\n",
    "\n",
    "#Decision tree showing immediate gains in all categories over GaussianNB.  \n",
    "#Will use for tuning\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script.\n",
    "\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(max_depth = 1) #1                       \n",
    "#Accuracy: 0.81478\tPrecision: 0.93701\tRecall: 0.17850\n",
    "\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(max_depth = 5) #2\n",
    "# Accuracy: 0.77133\tPrecision: 0.48155\tRecall: 0.37850\n",
    "\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(max_depth = 10) #3\n",
    "#Accuracy: 0.75467\tPrecision: 0.44439\tRecall: 0.41550\n",
    "\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(criterion = 'entropy') #4\n",
    "#Accuracy: 0.76611\tPrecision: 0.47042\tRecall: 0.41750\n",
    "\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(criterion ='entropy',max_depth = 1) #5\n",
    "#Accuracy: 0.81222\tPrecision: 0.93296\tRecall: 0.16700\n",
    "\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(criterion ='entropy',max_depth = 10) #6\n",
    "#Accuracy: 0.76800\tPrecision: 0.47511\tRecall: 0.42000\n",
    "\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(min_samples_leaf=5) #7\n",
    "#Accuracy: 0.76378\tPrecision: 0.46106\tRecall: 0.37300\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier(min_samples_leaf=2) #8\n",
    "#Accuracy: 0.78322\tPrecision: 0.51510\tRecall: 0.41800\n",
    "\n",
    "#Most all models meet requirements but I will use #8 as my final model\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n",
    "\n",
    "### Dump your classifier, dataset, and features_list so \n",
    "### anyone can run/check your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced version of poi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import test_classifier, dump_classifier_and_data\n",
    "import sklearn\n",
    "import remove_outlier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "import poi_interaction\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "features_list = ['poi','from_this_person_to_poi', 'from_poi_to_this_person', 'shared_receipt_with_poi'] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "outliers = ['TOTAL']\n",
    "remove_outlier.remove_outlier(data_dict, outliers)\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "my_dataset = copy(data_dict)\n",
    "features_list = copy(features_list)\n",
    "\n",
    "#Add new feature new features\n",
    "poi_interaction.add_poi_interaction(my_dataset, features_list)\n",
    "\n",
    "#Print features\n",
    "print \"{0} selected features: {1}\\n\".format(len(features_list) - 1, features_list[1:])\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "clf = sklearn.tree.DecisionTreeClassifier(min_samples_leaf=2) #8\n",
    "#Accuracy: 0.78322\tPrecision: 0.51510\tRecall: 0.41800\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n",
    "\n",
    "### Dump your classifier, dataset, and features_list so \n",
    "### anyone can run/check your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat\n",
    "from feature_format import targetFeatureSplit\n",
    "\n",
    "import enron\n",
    "import evaluate\n",
    "\n",
    "# features_list is a list of strings, each of which is a feature name\n",
    "# first feature must be \"poi\", as this will be singled out as the label\n",
    "target_label = 'poi'\n",
    "email_features_list = [\n",
    "    # 'email_address', # remit email address; informational label\n",
    "    'from_messages',\n",
    "    'from_poi_to_this_person',\n",
    "    'from_this_person_to_poi',\n",
    "    'shared_receipt_with_poi',\n",
    "    'to_messages',\n",
    "    ]\n",
    "financial_features_list = [\n",
    "    'bonus',\n",
    "    'deferral_payments',\n",
    "    'deferred_income',\n",
    "    'director_fees',\n",
    "    'exercised_stock_options',\n",
    "    'expenses',\n",
    "    'loan_advances',\n",
    "    'long_term_incentive',\n",
    "    'other',\n",
    "    'restricted_stock',\n",
    "    'restricted_stock_deferred',\n",
    "    'salary',\n",
    "    'total_payments',\n",
    "    'total_stock_value',\n",
    "]\n",
    "features_list = [target_label] + financial_features_list + email_features_list\n",
    "\n",
    "# load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"../data/final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "# remove outliers\n",
    "outlier_keys = ['TOTAL', 'THE TRAVEL AGENCY IN THE PARK', 'LOCKHART EUGENE E']\n",
    "enron.remove_keys(data_dict, outlier_keys)\n",
    "\n",
    "# instantiate copies of dataset and features for grading purposes\n",
    "my_dataset = copy(data_dict)\n",
    "my_feature_list = copy(features_list)\n",
    "\n",
    "# get K-best features\n",
    "num_features = 10 # 10 for logistic regression, 8 for k-means clustering\n",
    "best_features = enron.get_k_best(my_dataset, my_feature_list, num_features)\n",
    "my_feature_list = [target_label] + best_features.keys()\n",
    "\n",
    "# add two new features\n",
    "enron.add_financial_aggregate(my_dataset, my_feature_list)\n",
    "enron.add_poi_interaction(my_dataset, my_feature_list)\n",
    "\n",
    "# print features\n",
    "print \"{0} selected features: {1}\\n\".format(len(my_feature_list) - 1, my_feature_list[1:])\n",
    "\n",
    "# extract the features specified in features_list\n",
    "data = featureFormat(my_dataset, my_feature_list)\n",
    "\n",
    "# split into labels and features (this line assumes that the first\n",
    "# feature in the array is the label, which is why \"poi\" must always\n",
    "# be first in the features list\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# scale features via min-max\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# parameter optimization (not currently used)\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "### Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "l_clf = None\n",
    "###################################################\n",
    "# brute-force parameter optimizer; uncomment to run\n",
    "# TODO: use GridSearchCV\n",
    "# k = 0\n",
    "# best_combo = None\n",
    "# max_exponent = 21\n",
    "# for i in range(0, max_exponent, 3):\n",
    "#     for j in range(0, max_exponent, 3):\n",
    "#         print \"i: {0}, j: {1}\".format(i, j)\n",
    "#         l_clf = LogisticRegression(C=10**i, tol=10**-j, class_weight='auto')\n",
    "#         results = evaluate.evaluate_clf(l_clf, features, labels)\n",
    "#         if sum(results) > k:\n",
    "#             k = sum(results)\n",
    "#             best_combo = (i, j)\n",
    "# l_clf = LogisticRegression(C=10**i, tol=10**-j)\n",
    "###################################################\n",
    "if not l_clf:\n",
    "    l_clf = LogisticRegression(C=10**18, tol=10**-21)\n",
    "\n",
    "### K-means Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "k_clf = KMeans(n_clusters=2, tol=0.001)\n",
    "\n",
    "### Adaboost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "a_clf = AdaBoostClassifier(algorithm='SAMME')\n",
    "\n",
    "### Support Vector Machine Classifier\n",
    "from sklearn.svm import SVC\n",
    "s_clf = SVC(kernel='rbf', C=1000)\n",
    "\n",
    "### Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "### Stochastic Gradient Descent - Logistic Regression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "g_clf = SGDClassifier(loss='log')\n",
    "\n",
    "### Selected Classifiers Evaluation\n",
    "evaluate.evaluate_clf(l_clf, features, labels)\n",
    "evaluate.evaluate_clf(k_clf, features, labels)\n",
    "\n",
    "### Final Machine Algorithm Selection\n",
    "clf = l_clf\n",
    "\n",
    "# dump your classifier, dataset and features_list so\n",
    "# anyone can run/check your results\n",
    "pickle.dump(clf, open(\"../data/my_classifier.pkl\", \"w\"))\n",
    "pickle.dump(my_dataset, open(\"../data/my_dataset.pkl\", \"w\"))\n",
    "pickle.dump(my_feature_list, open(\"../data/my_feature_list.pkl\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import test_classifier, dump_classifier_and_data\n",
    "import sklearn\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "#Used gaussianNB to quickly determine features with highest accuracy\n",
    "\n",
    "'''\n",
    "Feature options cheat sheet\n",
    "\n",
    "bonus                     \n",
    "deferral_payments       \n",
    "deferred_income            \n",
    "director_fees              \n",
    "email_address               \n",
    "exercised_stock_options    \n",
    "expenses                    \n",
    "from_messages        \n",
    "from_poi_to_this_person     \n",
    "from_this_person_to_poi  \n",
    "loan_advances                \n",
    "long_term_incentive          \n",
    "other                        \n",
    "poi                        \n",
    "restricted_stock            \n",
    "restricted_stock_deferred    \n",
    "salary                       \n",
    "shared_receipt_with_poi     \n",
    "to_messages                  \n",
    "total_payments               \n",
    "total_stock_value            \n",
    "'''\n",
    "\n",
    "#features_list = ['poi', 'salary', 'bonus'] #1 \n",
    "#features_list = ['poi', 'salary', 'bonus', 'from_this_person_to_poi'] #2\n",
    "#features_list = ['poi', 'salary', 'from_poi_to_this_person', 'from_this_person_to_poi'] #3\n",
    "features_list = ['poi','from_this_person_to_poi', 'from_poi_to_this_person', 'shared_receipt_with_poi'] #4\n",
    "#features_list = ['poi','from_this_person_to_poi'] #5\n",
    "\n",
    "#Gaussian test\n",
    "#1 = Accuracy: 0.24480\tPrecision: 0.18368\tRecall: 0.80600\n",
    "#2 = Accuracy: 0.23282\tPrecision: 0.15703\tRecall: 0.73700\n",
    "#3 = Accuracy: 0.58418\tPrecision: 0.16291\tRecall: 0.31100\n",
    "#4 = Accuracy: 0.71678\tPrecision: 0.01757\tRecall: 0.00500\n",
    "#5 = Accuracy: 0.63971\tPrecision: 0.12392\tRecall: 0.04300\tF1: 0.06385\n",
    "\n",
    "#4 = highest accuracy, try different classifier tunes to improve precision and recall\n",
    "\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "\n",
    "#clf = GaussianNB() \n",
    "#4 = Accuracy: 0.71678\tPrecision: 0.01757\tRecall: 0.00500\n",
    "#clf = sklearn.tree.DecisionTreeClassifier()\n",
    "#Accuracy: 0.73856\tPrecision: 0.39827\tRecall: 0.34550\n",
    "\n",
    "#Decision tree showing immediate gains in all categories over GaussianNB.  \n",
    "#Will use for tuning\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script.\n",
    "### Because of the small size of the dataset, the script uses stratified\n",
    "### shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(max_depth = 1) #1                       \n",
    "#Accuracy: 0.81478\tPrecision: 0.93701\tRecall: 0.17850  \n",
    "#clf = sklearn.tree.DecisionTreeClassifier(max_depth = 5) #2\n",
    "# Accuracy: 0.73078\tPrecision: 0.37174\tRecall: 0.30650\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(max_depth = 10) #3\n",
    "#Accuracy: 0.73756\tPrecision: 0.39716\tRecall: 0.34950\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(criterion = 'entropy') #4\n",
    "#Accuracy: 0.73989\tPrecision: 0.40606\tRecall: 0.36850\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(criterion ='entropy',max_depth = 1) #5\n",
    "#Accuracy: 0.81222\tPrecision: 0.93296\tRecall: 0.16700\n",
    "clf = sklearn.tree.DecisionTreeClassifier(criterion ='entropy',max_depth = 10) #6\n",
    "#Accuracy: 0.74000\tPrecision: 0.40207\tRecall: 0.34900\n",
    "#clf = sklearn.tree.DecisionTreeClassifier(min_samples_leaf=5) #7\n",
    "#Accuracy: 0.72878\tPrecision: 0.38036\tRecall: 0.35050   \n",
    "#clf = sklearn.tree.DecisionTreeClassifier(min_samples_leaf=2) #8\n",
    "#Accuracy: 0.73822\tPrecision: 0.38648\tRecall: 0.30300\n",
    "\n",
    "#Little improvement from #6 but consistently better without too much complexity\n",
    "#Favorite is #5 but doesn't meet .3 recall. Use #5 \n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n",
    "\n",
    "### Dump your classifier, dataset, and features_list so \n",
    "### anyone can run/check your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
